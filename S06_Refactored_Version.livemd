# S06 - Analyse Lexicale - Partie 3 (Mini-Project) - fork

## Info étudiant

* étudiant : *<prénom+nom ici>*

## Instructions

* Date butoir : Jeudi **dans deux semaines** à 05h00
* Travail à rendre : une copie de votre livebook pour le mini-projet au format `.livemd`,
  incluant la documentation de votre code intégrée celui-ci, et votre retour théorique personnel.
* N'oubliez pas de mettre votre nom au début du fichier

## Exercice 1

Dans cet exercice, nous terminons l'analyse lexicale commencée dans l'exercice 3 de la série 05.

Les tokens reconnus pour ce langage sont :

* `begin_block` qui est exprimé par le caractère `"{"`
* `end_block` qui est exprimé par le caractère `"}"`
* `begin_par` qui est exprimé par le caractère `"("`
* `end_par` qui est exprimé par le caractère `")"`
* `semicolon` qui est exprimé par le caractère `";"`
* `op_eg` qui est exprimé par la suite de caractère `"=="`
* `op_affect` qui est exprimé par le caractère `"="`
* `op_add` qui est exprimé par le caractère `"+"`
* `op_minus` qui est exprimé par le caractère `"-"`
* `op_mult` qui est exprimé par le caractère `"*"`
* `op_div` qui est exprimé par le caractère `"/"`
* `type_int` qui est exprimé par la suite de caractères `"int"`
* `cond` qui est exprimé par la suite de caractères `"if"`
* `loop` qui est exprimé par la suite de caractères `"while"`
* `value_int` qui est exprimé par une suite de caractère respectant l'expression
  régulière `[0-9]+`
* `ident` qui est exprimé par une suite de caractère respectant l'expression régulière
  `[A-Za-z_][A-Za-z0-9_]*`

<!-- livebook:{"break_markdown":true} -->

### Typespec

Les types introduits dans la série 05 sont en partie repris et transformés comme suit :

* Un automate est modélisé par une structure : l'état initial de l'automate, la liste de ses
  états finaux, et la mappe de ses transitions

  ```elixir
  @type t :: %Automaton{
    initial: valid_state(),
    finals: [valid_state(), ...],
    transitions: transitions()
  }
  ```

  * ```elixir
    @type valid_state :: integer()
    ```
  * ```elixir
    @type state :: valid_state() | nil
    ```
  * ```elixir
    @type predicate :: (char() -> boolean())
    ```
  * ```elixir
    @type transitions :: %{valid_state() => [{predicate(), valid_state()}, ...]}
    ```

* Un token est modélisé par une structure : son code et l'automate modélisant le token

  ```elixir
  @type t :: %Token{code: code(), automaton: Automaton.t()}
  ```

  * ```elixir
    @type code :: atom()
    ```

* Un token reconnu est modélisé par une doublet : son code et le mot reconnu

  ```elixir
  @type recognized_token :: {Token.code(), String.t()}
  ```

#### Important

La simplification adoptée dans la série 05 supposant que les tokens du texte à analyser sont
séparés par des espaces est abandonnée.

Les tokens du texte à reconnaître ne sont donc plus forcément séparés par des espaces, par
exemple :

`"int x; int y; x=y*2;"`

<!-- livebook:{"break_markdown":true} -->

### Travail à faire

#### Représentez chacun des tokens de ce langage, par exemple :

* `begin_block` est représenté par la structure :
  ```elixir
  t01 = Token.new(:begin_block, Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]}))
  ```

  * `Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]})` est l'automate qui reconnaît
    la chaîne de caractère `"{"`.
  * `%{0 => [{&(&1 == ?{), 1}]}` est la seule transition de cet automate. Son état de
    départ est l'état `0`, son état d'arrivée est l'état `1` et le caractère porté
    satisfait le prédicat `&(&1 == ?{)`.

#### Développez les fonctions qui suivent :

* Reprenez et adaptez au besoin les fonctions `is_final_state/2` et `next_state/3` de
  l'exercice 2 de la série 05 comme fonctions **auxiliaires**.

* `recognized_from_state(state, text, acc, token)` : cette fonction **auxiliaire**
  cherche à extraire du texte `text` un token en utilisant l'automate à état fini du
  `token`, l'état de départ `state` et l'accumulateur `acc` contenant un doublet
  `{rec, length}` - la partie du texte reconnue `rec` et sa longueur `length`.

  Elle retourne un quadruplet `{:ok, rec_token, length, rest}` où `rec_token` est le
  doublet `{code, rec}` - le `code` du token et la partie du texte reconnue `rec` -
  `length` est la longueur de la partie du texte reconnue par le token et `rest` est
  la partie du texte qui suit. Elle retourne `:error` si le début de `text` n'est pas
  reconnu par le token. Cette fonction est "gloutonne", car elle reconnaît la plus grande
  partie possible de texte.

  * Exemple : si `t16` est la structure `%Token{}` permettant de reconnaître des
    identificateurs, cela signifie que l'on cherche si le début du texte est reconnu par
    le token :

    `recognized_from_state(0, ~c"toto=3;", {~c"", 0}, t16)` retourne le quadruplet
    `{:ok, {:ident, "toto"}, 4, ~c"=3;"}` où `{:ident, "toto"}` est le token reconnu par
    `t16`, `4` est la longueur car `~c"toto"` est le plus grand identificateur au début du
    texte `~c"toto=3;"` et `~c"=3;"` est la partie du texte située après la partie
    reconnue.

    `recognized_from_state(0, ~c"{x=3;}", {~c"", 0}, t16)` retourne `:error` car le texte
    ne commence pas par un identificateur.

* `fetch_recognized_token(text, tokens)` : cette fonction recherche quel est le token qui
  reconnaît le début du texte `text` avec la liste des tokens du langage `tokens`.

  Elle retourne un triplet `{:ok, rec_token, rest}` où `rec_token` est le doublet
  `{code, rec}` - le `code` du token reconnu et la partie du texte reconnue `rec` - et
  `rest` est la partie du texte qui suit la partie du texte reconnu. Elle retourne `:error`
  si le texte n'est reconnu par aucun des tokens du langage.

  * Exemple : si `lt` est la liste des tokens du langage :

    `fetch_recognized_token(~c"{x=3;}", lt)` retourne le triplet
    `{:ok, {:begin_block, "{"}, ~c"x=3;}"}` car le texte commence par le token `~c"{"`, et
    `~c"x=3;}"` est la partie du texte qui suit.

    `fetch_recognized_token(~c": x=3;", lt)` retourne `:error` car le texte commence par
    le token `~c":"` qui n'est reconnu par aucun des tokens.

  * **Attention** : `fetch_recognized_token(~c"while (x==3)", lt)` doit retourner le
    triplet `{:ok, {:loop, "while"}, ~c" (x==3)"}` et non le triplet
    `{:ok, {:ident, "while"}, ~c" (x==3)"}`.

  * Indice : l'ordre dans lequel les tokens sont testés par cette fonction est
    significatif. Le token reconnu est celui qui reconnaît le plus long mot. Si deux tokens
    reconnaissent le même mot le gagnant est le premier qui a été testé.

* `lex_analysis!(text, tokens)` : cette fonction retourne la liste des codes des tokens
  composant le texte `text`. Elle crée une exception s'il y a une partie qui n'est
  reconnue par aucun des tokens du langage.

  * Exemple : `lex_analysis!("toto=3;", lt)` retourne la liste

    ```elixir
    [ident: "toto", op_affect: "=", value_int: "3", semicolon: ";"]
    ```

  * Indice : vous pouvez implémenter et utiliser une fonction **auxiliaire** `trim(text)`
    qui retourne la chaîne de caractères `text` sans ses premiers caractères qui sont des
    espaces.

    * Exemple : `trim(~c" int x = y;")` retourne `~c"int x = y;"`.

```elixir
# Disclaimer: Parts of this code and its documentation were scaffolded and challenged 
# with the help of ChatGPT.
# 
# Special thanks also to Alonso for his extensive guidance and help.

defmodule Token do
  @moduledoc """
  Represents a 'Token' struct, ideal for feeding a lexer.
  
  Token.new/2 creates a new token
  """

  @typedoc "An atom representing the type of a token"
  @type code :: atom()
  
  @typedoc "A token that has been recognized by the lexer."
  @type recognized_token :: {Token.code(), String.t()}

  @enforce_keys [:code, :automaton]
  defstruct [:code, :automaton]

  @typedoc "The 'Token' struct, representing a lexical token."
  @type t :: %Token{
    code: code(),
    automaton: Automaton.t()
  }

  @doc """
  Constructor to create a new 'Token'. Entails a code and an 'Automaton'.
  """
  @spec new(code(), Automaton.t()) :: t()
  def new(code, automaton) do
    %Token{
      code: code,
      automaton: automaton
    }
  end
end

defmodule Automaton do
  @moduledoc """
  Represents an 'Automaton' struct, to be included in a 'Token' struct.
  
  Automaton.new/3 creates a new 'Automaton' with initial and final state(s) plus transitions.
  """
  
  @typedoc "An integer representing a valid automaton state."
  @type valid_state :: integer()

  @typedoc "A state that can be valid or nil."
  @type state :: valid_state() | nil

  @typedoc "A function taking a character and returning a boolean."
  @type predicate :: (char() -> boolean())

  @typedoc "A map of states to non-empty lists of {predicate, target_state} tuples."
  @type transitions :: %{valid_state() => [{predicate(), valid_state()}, ...]}

  @enforce_keys [:initial, :finals, :transitions]
  defstruct [:initial, :finals, :transitions]

  @typedoc "Create a new automaton with initial, final state(s), and transitions."
  @type t :: %Automaton{
    initial: valid_state(),
    finals: [valid_state(), ...],
    transitions: transitions()
  }

  @doc """
  Constructor to create a new 'Automaton'. Entails an initial state, final state(s) and transitions.
  """
  @spec new(initial :: valid_state(), finals :: [valid_state(), ...], transitions :: transitions()) :: t()
  def new(initial, finals, transitions) do
  %Automaton{
      initial: initial,
      finals: finals,
      transitions: transitions
    }
  end
end

tokens = [
      # Defines the list of tokens recognized by the lexer.
      # - Brackets / delimiters
      Token.new(:begin_block, Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]})),
      Token.new(:end_block,   Automaton.new(0, [1], %{0 => [{&(&1 == ?}), 1}]})),
      Token.new(:begin_par,   Automaton.new(0, [1], %{0 => [{&(&1 == ?(), 1}]})),
      Token.new(:end_par,     Automaton.new(0, [1], %{0 => [{&(&1 == ?)), 1}]})),
      Token.new(:semicolon,   Automaton.new(0, [1], %{0 => [{&(&1 == ?;), 1}]})),

      # - Operators
      Token.new(:op_eg,       Automaton.new(0, [2], %{0 => [{&(&1 == ?=), 1}], 
                                                      1 => [{&(&1 == ?=), 2}]})),
      Token.new(:op_affect,   Automaton.new(0, [1], %{0 => [{&(&1 == ?=), 1}]})),
      Token.new(:op_add,      Automaton.new(0, [1], %{0 => [{&(&1 == ?+), 1}]})),
      Token.new(:op_minus,    Automaton.new(0, [1], %{0 => [{&(&1 == ?-), 1}]})),
      Token.new(:op_mult,     Automaton.new(0, [1], %{0 => [{&(&1 == ?*), 1}]})),
      Token.new(:op_div,      Automaton.new(0, [1], %{0 => [{&(&1 == ?/), 1}]})),

      # - Keywords
      Token.new(:type_int,    Automaton.new(0, [3], %{0 => [{&(&1 == ?i), 1}], 
                                                      1 => [{&(&1 == ?n), 2}], 
                                                      2 => [{&(&1 == ?t), 3}]})),
      Token.new(:cond,        Automaton.new(0, [2], %{0 => [{&(&1 == ?i), 1}], 
                                                      1 => [{&(&1 == ?f), 2}]})),
      Token.new(:loop,        Automaton.new(0, [5], %{0 => [{&(&1 == ?w), 1}], 
                                                      1 => [{&(&1 == ?h), 2}], 
                                                      2 => [{&(&1 == ?i), 3}], 
                                                      3 => [{&(&1 == ?l), 4}], 
                                                      4 => [{&(&1 == ?e), 5}]})),
      # - Literals / identifiers
      Token.new(:value_int,   Automaton.new(0, [1], %{0 => [{&(&1 in ?0..?9), 1}], 
                                                      1 => [{&(&1 in ?0..?9), 1}]})),
      Token.new(:ident,       Automaton.new(0, [1], %{0 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 == ?_), 1}], 
                                                      1 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 in ?0..?9 or &1 == ?_), 1}]}))
]

defmodule Lexer do
  @moduledoc """
  A lexical analyzer (lexer) for tokenizing text based on a set of defined tokens.

  This module provides functions to recognize tokens in a text using 
  finite automata associated with each token. It supports:

    * 'next_state/3'
    * 'recognized_from_state/4'
    * 'fetch_recognized_token/2'
    * 'lex_analysis!/2'
  """

  @doc """
  Computes the next states of an automaton given a current 'state' and input character 'c'.
  
  Returns a list of states reachable from 'state' via transitions whose predicate
  matches 'c'. Returns an empty list if no transitions exist for the given character.
  """
  @spec next_state(state :: Automaton.valid_state(), c :: char(), automaton :: Automaton.t()) :: [Automaton.valid_state()]
  def next_state(state, c, automaton) do
    transitions = Map.get(automaton.transitions , state, []) # get transitions of automaton

    # list comprehension, filtered: only keep tuples if predicate.(c) returns true
    for {predicate, next_state} <- transitions, predicate.(c) do
      next_state
    end
  end

  @doc """
  Attempts to recognize a token from the given 'state' and 'text' using the token's automaton.
  
  Returns a quadruple '{:ok, {token_code, recognized_chars}, length, rest}' where:
  
    * 'token_code' — the code of the recognized token
    * 'recognized_chars' — the list of characters matched
    * 'length' — the number of characters consumed
    * 'rest' — the remaining unparsed text
  
  Otherwise it returns ':error' if no characters are matched.
  """
  @spec recognized_from_state(state :: Automaton.valid_state(),text :: [char()],acc :: {list(char()), non_neg_integer()},token :: Token.t()) ::
        {:ok, {Token.code(), [char()]}, non_neg_integer(), [char()]} | :error
  def recognized_from_state(state, text, {rec, len}, token) do
    {_, rec, length} =
      Enum.reduce_while(text, {state, rec, len}, fn char, {state, rec, len} ->
        next = next_state(state, char, token.automaton) # get next state 
          
        if next == [] do
          {:halt, {state, rec, len}}                    # stop immediately when next is empty...
        else
          #IO.inspect("Process char #{<<char>>} -> next state: #{hd(next)}")
          {:cont, {hd(next), rec ++ [char], len + 1}}   # ... otherwise continue accumulating
        end
      end)
    
    rest = Enum.drop(text, length)                      # return the rest of the unparsed text
    
    if length == 0 do                                   # return :error if nothing is parsed
      :error
    else
      {:ok, { token.code, List.to_string(rec) }, length, rest}          # otherwise return resulting quadruple
    end
  end

  
  @doc """
  Finds the token from the given list of 'tokens' that recognizes the longest
  prefix of 'text'.  
  
  Returns '{:ok, {token_code, recognized_chars}, length, rest}' if a token is recognized,
  or ':error' if no token matches.
  """
  @spec fetch_recognized_token(text :: [char()],tokens :: [Token.t()]) ::
        {:ok, {Token.code(), [char()]}, non_neg_integer(), [char()]} | :error
  def fetch_recognized_token(text, tokens) do
      Enum.map(tokens, fn token ->        # invoke recognized_from_state on every token 
        recognized_from_state(0, text, {~c"", 0}, token)
        end)
      |> Enum.filter(fn
        {:ok, _, _, _} -> true            # filter out only successful tokens
        _ -> false
      end)
      |> Enum.sort_by(fn {:ok, _, length, _rest} -> length end, &>=/2) # sort tokens by length (desc)
      |> List.first()                     # take the first and thus longest token
      |> (fn                             # when tuple is not nil: remove length & shorten tuple
        nil -> nil
        {a, {b1, b2}, _, d} -> {a, {b1, b2}, d}
      end).()
      |> case do
       nil -> :error                      # return :error when result is empty ...
       matches -> matches                 # ... otherwise found results (=matches)     
     end
  end

  @doc """
  Recursively tokenizes the input 'text' using the list of 'tokens'.
  
  Converts the input string to a charlist, repeatedly finds the longest
  matching token at the beginning of the text, and returns a flat list
  of token codes and their recognized string values.
  """
  @spec lex_analysis!(text :: String.t(), tokens :: [Token.t()]) :: [Token.code() | String.t()]
  def lex_analysis!("", _), do: []
  def lex_analysis!(text, tokens) do
    text = String.to_charlist(String.trim(text))
    {_,{ident, rec}, _, rest} = fetch_recognized_token(text, tokens) # fetch
    [ident, List.to_string(rec) | lex_analysis!(List.to_string(rest), tokens)] # and recurse rest
  end
end


# doit retourner le triplet {:ok, {:loop, "while"}, ~c" (x==3)"} et non le triplet {:ok, {:ident, "while"}, ~c" (x==3)"}.
Lexer.fetch_recognized_token(~c"while (x==3)", tokens) # ✔️

Lexer.fetch_recognized_token(~c"whilex (x==3)", tokens) # ✔️
```

```elixir
ExUnit.start()

defmodule LexerDoctest do
  @moduledoc ~S"""
  Module 'LexerDoctest' tests Lexer functions thoroughly.
  """

  # It's necessary to expose tokens via function as doctest has special needs.
  def tokens do
    [
    # Brackets / delimiters
    Token.new(:begin_block, Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]})),
    Token.new(:end_block,   Automaton.new(0, [1], %{0 => [{&(&1 == ?}), 1}]})),
    Token.new(:begin_par,   Automaton.new(0, [1], %{0 => [{&(&1 == ?(), 1}]})),
    Token.new(:end_par,     Automaton.new(0, [1], %{0 => [{&(&1 == ?)), 1}]})),
    Token.new(:semicolon,   Automaton.new(0, [1], %{0 => [{&(&1 == ?;), 1}]})),

    # Operators
    Token.new(:op_eg,       Automaton.new(0, [2], %{0 => [{&(&1 == ?=), 1}], 1 => [{&(&1 == ?=), 2}]})),
    Token.new(:op_affect,   Automaton.new(0, [1], %{0 => [{&(&1 == ?=), 1}]})),
    Token.new(:op_add,      Automaton.new(0, [1], %{0 => [{&(&1 == ?+), 1}]})),
    Token.new(:op_minus,    Automaton.new(0, [1], %{0 => [{&(&1 == ?-), 1}]})),
    Token.new(:op_mult,     Automaton.new(0, [1], %{0 => [{&(&1 == ?*), 1}]})),
    Token.new(:op_div,      Automaton.new(0, [1], %{0 => [{&(&1 == ?/), 1}]})),

    # Keywords
    Token.new(:type_int,    Automaton.new(0, [3], %{0 => [{&(&1 == ?i), 1}], 1 => [{&(&1 == ?n), 2}], 2 => [{&(&1 == ?t), 3}]})),
    Token.new(:cond,        Automaton.new(0, [2], %{0 => [{&(&1 == ?i), 1}], 1 => [{&(&1 == ?f), 2}]})),
    Token.new(:loop,        Automaton.new(0, [5], %{0 => [{&(&1 == ?w), 1}], 1 => [{&(&1 == ?h), 2}], 2 => [{&(&1 == ?i), 3}], 3 => [{&(&1 == ?l), 4}], 4 => [{&(&1 == ?e), 5}]})),

    # Literals / identifiers
    Token.new(:value_int,   Automaton.new(0, [1], %{0 => [{&(&1 in ?0..?9), 1}], 1 => [{&(&1 in ?0..?9), 1}]})),
    Token.new(:ident,       Automaton.new(0, [1], %{0 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 == ?_), 1}], 1 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 in ?0..?9 or &1 == ?_), 1}]}))
    ] end


  @moduledoc ~S"""
  Doctests for Lexer module:

    # Recognise simple ident
      iex> Lexer.recognized_from_state(0, ~c"toto=3;", {~c"", 0}, Enum.at(LexerDoctest.tokens(),15))
      {:ok, {:ident, "toto"}, 4, ~c"=3;"}

    # Recognise an error if ident block contains unallowed chars
      iex> Lexer.recognized_from_state(0, ~c"{x=3;}", {~c"", 0}, Enum.at(LexerDoctest.tokens(),15))
      :error

    # Recognise proper begin block
      iex> Lexer.recognized_from_state(0, ~c"{", {~c"", 0}, Enum.at(LexerDoctest.tokens(),0))
      {:ok, {:begin_block, "{"}, 1, []}

    # Has to recognise {:ok, {:loop, "while"}, ~c" (x==3)"} and not {:ok, {:ident, "while"}, ~c" (x==3)"}.
      iex> Lexer.fetch_recognized_token(~c"while (x==3)", LexerDoctest.tokens())
      {:ok, {:loop, "while"}, ~c" (x==3)"}
    
    # Recognition of unproper begin block
      iex> Lexer.recognized_from_state(0, ~c"{abc", {~c"", 0}, Enum.at(LexerDoctest.tokens(),0))
      {:ok, {:begin_block, "{"}, 1, ~c"abc"}

    # Recognition of unproper end block
      iex> Lexer.recognized_from_state(0, ~c"}toto=3;", {~c"", 0}, Enum.at(LexerDoctest.tokens(),1))
      {:ok, {:end_block, "}"}, 1, ~c"toto=3;"}

    # Recognise unallowed chars
      iex> Lexer.recognized_from_state(0, ~c"#abc", {~c"", 0}, Enum.at(LexerDoctest.tokens(),0))
      :error

    # Fetch proper ident token
      iex> Lexer.fetch_recognized_token(~c"toto", LexerDoctest.tokens())
      {:ok, {:ident, "toto"}, []}

    # Fetch begin block
      iex> Lexer.fetch_recognized_token(~c"{x=3;}", LexerDoctest.tokens())
      {:ok, {:begin_block, "{"}, ~c"x=3;}"}

    # Throw an error when fetching begins with ":"
      iex> Lexer.fetch_recognized_token(~c": x=3;", LexerDoctest.tokens())
      :error

    # Test order of processing the tokens by fetching triplet {:ok, {:loop, "while"}, ~c" (x==3)"} and
    # not triplet {:ok, {:ident, "while"}, ~c" (x==3)"}
      iex> Lexer.fetch_recognized_token(~c"while (x==3)", LexerDoctest.tokens())
      {:ok, {:loop, "while"}, ~c" (x==3)"}

    # Whereas a longer word is treated first
      iex> Lexer.fetch_recognized_token(~c"whileX (x==3)", LexerDoctest.tokens())
      {:ok, {:ident, "whileX"}, ~c" (x==3)"}
    
  """
end
```

#### Ajoutez de la documentation à votre code :

Commentez chaque module, fonction et type en utilisant respectivement
[`@moduledoc`](https://hexdocs.pm/elixir/writing-documentation.html), `@spec` +
[`@doc`](https://hexdocs.pm/elixir/writing-documentation.html) +
[Doctests](https://hexdocs.pm/elixir/docs-tests-and-with.html#doctests) et
[`@typedoc`](https://hexdocs.pm/elixir/typespecs.html#a-simple-example).

* Ajoutez des descriptions aux modules, types et fonctions publiques.
* Pour les fonctions, décrivez également leurs paramètres et leurs valeurs de retour.

##### Exemple

<!-- livebook:{"force_markdown":true} -->

```elixir
defmodule Foo do
  @moduledoc ~S"""
  This is the `Foo` module.

  It ingests anything with the `Foo.bar/1` function to produce baz stamps.
  """

  @typedoc ~s"A baz stamp"
  @type baz() :: :baz

  @doc ~S"""
  Ingest the given `input` to produce a baz stamp.

  Returns `:baz`.

  ## Examples

  These are examples of doctests:

      iex> Foo.bar("stamp")
      :baz

      iex> Foo.bar(0)
      :baz

      iex> Foo.bar(%{foo: :bar})
      :baz
  """
  @spec bar(any()) :: baz()
  def bar(_input), do: :baz
end
```

<!-- livebook:{"break_markdown":true} -->

### Retour théorique personnel

* Expliquez et justifiez les choix d’implémentation concernant vos fonctions
  pour cette dernière version du projet. Proposer des possibles ouvertures
  afin de continuer ce projet, ou pour l’adapter à l’analyse d’un autre
  langage. (~250 mots)
* Faites un retour réflexif sur le langage Elixir (ses points forts / faibles, les
  techniques propre à la programmation fonctionnelle, …). Comparez avec
  des langages et paradigmes que vous connaissez déjà. (~250 mots)

<!-- livebook:{"break_markdown":true} -->

*Votre rapport ici*
