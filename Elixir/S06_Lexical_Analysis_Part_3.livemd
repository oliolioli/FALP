# S06 - Analyse Lexicale - Partie 3 (Mini-Project) - fork

## Info étudiant

Olivier Meier

## Instructions

* Date butoir : Jeudi **dans deux semaines** à 05h00
* Travail à rendre : une copie de votre livebook pour le mini-projet au format `.livemd`,
  incluant la documentation de votre code intégrée celui-ci, et votre retour théorique personnel.
* N'oubliez pas de mettre votre nom au début du fichier

## Exercice 1

Dans cet exercice, nous terminons l'analyse lexicale commencée dans l'exercice 3 de la série 05.

Les tokens reconnus pour ce langage sont :

* `begin_block` qui est exprimé par le caractère `"{"`
* `end_block` qui est exprimé par le caractère `"}"`
* `begin_par` qui est exprimé par le caractère `"("`
* `end_par` qui est exprimé par le caractère `")"`
* `semicolon` qui est exprimé par le caractère `";"`
* `op_eg` qui est exprimé par la suite de caractère `"=="`
* `op_affect` qui est exprimé par le caractère `"="`
* `op_add` qui est exprimé par le caractère `"+"`
* `op_minus` qui est exprimé par le caractère `"-"`
* `op_mult` qui est exprimé par le caractère `"*"`
* `op_div` qui est exprimé par le caractère `"/"`
* `type_int` qui est exprimé par la suite de caractères `"int"`
* `cond` qui est exprimé par la suite de caractères `"if"`
* `loop` qui est exprimé par la suite de caractères `"while"`
* `value_int` qui est exprimé par une suite de caractère respectant l'expression
  régulière `[0-9]+`
* `ident` qui est exprimé par une suite de caractère respectant l'expression régulière
  `[A-Za-z_][A-Za-z0-9_]*`

<!-- livebook:{"break_markdown":true} -->

### Typespec

Les types introduits dans la série 05 sont en partie repris et transformés comme suit :

* Un automate est modélisé par une structure : l'état initial de l'automate, la liste de ses
  états finaux, et la mappe de ses transitions

  ```elixir
  @type t :: %Automaton{
    initial: valid_state(),
    finals: [valid_state(), ...],
    transitions: transitions()
  }
  ```

  * ```elixir
    @type valid_state :: integer()
    ```
  * ```elixir
    @type state :: valid_state() | nil
    ```
  * ```elixir
    @type predicate :: (char() -> boolean())
    ```
  * ```elixir
    @type transitions :: %{valid_state() => [{predicate(), valid_state()}, ...]}
    ```

* Un token est modélisé par une structure : son code et l'automate modélisant le token

  ```elixir
  @type t :: %Token{code: code(), automaton: Automaton.t()}
  ```

  * ```elixir
    @type code :: atom()
    ```

* Un token reconnu est modélisé par une doublet : son code et le mot reconnu

  ```elixir
  @type recognized_token :: {Token.code(), String.t()}
  ```

#### Important

La simplification adoptée dans la série 05 supposant que les tokens du texte à analyser sont
séparés par des espaces est abandonnée.

Les tokens du texte à reconnaître ne sont donc plus forcément séparés par des espaces, par
exemple :

`"int x; int y; x=y*2;"`

<!-- livebook:{"break_markdown":true} -->

### Travail à faire

#### Représentez chacun des tokens de ce langage, par exemple :

* `begin_block` est représenté par la structure :
  ```elixir
  t01 = Token.new(:begin_block, Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]}))
  ```

  * `Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]})` est l'automate qui reconnaît
    la chaîne de caractère `"{"`.
  * `%{0 => [{&(&1 == ?{), 1}]}` est la seule transition de cet automate. Son état de
    départ est l'état `0`, son état d'arrivée est l'état `1` et le caractère porté
    satisfait le prédicat `&(&1 == ?{)`.

#### Développez les fonctions qui suivent :

* Reprenez et adaptez au besoin les fonctions `is_final_state/2` et `next_state/3` de
  l'exercice 2 de la série 05 comme fonctions **auxiliaires**.

* `recognized_from_state(state, text, acc, token)` : cette fonction **auxiliaire**
  cherche à extraire du texte `text` un token en utilisant l'automate à état fini du
  `token`, l'état de départ `state` et l'accumulateur `acc` contenant un doublet
  `{rec, length}` - la partie du texte reconnue `rec` et sa longueur `length`.

  Elle retourne un quadruplet `{:ok, rec_token, length, rest}` où `rec_token` est le
  doublet `{code, rec}` - le `code` du token et la partie du texte reconnue `rec` -
  `length` est la longueur de la partie du texte reconnue par le token et `rest` est
  la partie du texte qui suit. Elle retourne `:error` si le début de `text` n'est pas
  reconnu par le token. Cette fonction est "gloutonne", car elle reconnaît la plus grande
  partie possible de texte.

  * Exemple : si `t16` est la structure `%Token{}` permettant de reconnaître des
    identificateurs, cela signifie que l'on cherche si le début du texte est reconnu par
    le token :

    `recognized_from_state(0, ~c"toto=3;", {~c"", 0}, t16)` retourne le quadruplet
    `{:ok, {:ident, "toto"}, 4, ~c"=3;"}` où `{:ident, "toto"}` est le token reconnu par
    `t16`, `4` est la longueur car `~c"toto"` est le plus grand identificateur au début du
    texte `~c"toto=3;"` et `~c"=3;"` est la partie du texte située après la partie
    reconnue.

    `recognized_from_state(0, ~c"{x=3;}", {~c"", 0}, t16)` retourne `:error` car le texte
    ne commence pas par un identificateur.

* `fetch_recognized_token(text, tokens)` : cette fonction recherche quel est le token qui
  reconnaît le début du texte `text` avec la liste des tokens du langage `tokens`.

  Elle retourne un triplet `{:ok, rec_token, rest}` où `rec_token` est le doublet
  `{code, rec}` - le `code` du token reconnu et la partie du texte reconnue `rec` - et
  `rest` est la partie du texte qui suit la partie du texte reconnu. Elle retourne `:error`
  si le texte n'est reconnu par aucun des tokens du langage.

  * Exemple : si `lt` est la liste des tokens du langage :

    `fetch_recognized_token(~c"{x=3;}", lt)` retourne le triplet
    `{:ok, {:begin_block, "{"}, ~c"x=3;}"}` car le texte commence par le token `~c"{"`, et
    `~c"x=3;}"` est la partie du texte qui suit.

    `fetch_recognized_token(~c": x=3;", lt)` retourne `:error` car le texte commence par
    le token `~c":"` qui n'est reconnu par aucun des tokens.

  * **Attention** : `fetch_recognized_token(~c"while (x==3)", lt)` doit retourner le
    triplet `{:ok, {:loop, "while"}, ~c" (x==3)"}` et non le triplet
    `{:ok, {:ident, "while"}, ~c" (x==3)"}`.

  * Indice : l'ordre dans lequel les tokens sont testés par cette fonction est
    significatif. Le token reconnu est celui qui reconnaît le plus long mot. Si deux tokens
    reconnaissent le même mot le gagnant est le premier qui a été testé.

* `lex_analysis!(text, tokens)` : cette fonction retourne la liste des codes des tokens
  composant le texte `text`. Elle crée une exception s'il y a une partie qui n'est
  reconnue par aucun des tokens du langage.

  * Exemple : `lex_analysis!("toto=3;", lt)` retourne la liste

    ```elixir
    [ident: "toto", op_affect: "=", value_int: "3", semicolon: ";"]
    ```

  * Indice : vous pouvez implémenter et utiliser une fonction **auxiliaire** `trim(text)`
    qui retourne la chaîne de caractères `text` sans ses premiers caractères qui sont des
    espaces.

    * Exemple : `trim(~c" int x = y;")` retourne `~c"int x = y;"`.

```elixir
# Disclaimer: Parts of this code and its documentation were scaffolded and challenged 
# with the help of ChatGPT.
# 
# DeepL was used in places to translate the personal reflection.
# 
# Special thanks also to Alonso for his extensive guidance and help.

defmodule Token do
  @moduledoc """
  Represents a 'Token' struct, ideal for feeding a lexer.
  
  Token.new/2 creates a new token
  """

  @typedoc "An atom representing the type of a token"
  @type code :: atom()
  
  @typedoc "A token that has been recognized by the lexer."
  @type recognized_token :: {Token.code(), String.t()}

  @enforce_keys [:code, :automaton]
  defstruct [:code, :automaton]

  @typedoc "The 'Token' struct, representing a lexical token."
  @type t :: %Token{
    code: code(),
    automaton: Automaton.t()
  }

  @doc """
  Constructor to create a new 'Token'. Entails a code and an 'Automaton'.
  """
  @spec new(code(), Automaton.t()) :: t()
  def new(code, automaton) do
    %Token{
      code: code,
      automaton: automaton
    }
  end
end

defmodule Automaton do
  @moduledoc """
  Represents an 'Automaton' struct, to be included in a 'Token' struct.
  
  Automaton.new/3 creates a new 'Automaton' with initial and final state(s) plus transitions.
  """
  
  @typedoc "An integer representing a valid automaton state."
  @type valid_state :: integer()

  @typedoc "A state that can be valid or nil."
  @type state :: valid_state() | nil

  @typedoc "A function taking a character and returning a boolean."
  @type predicate :: (char() -> boolean())

  @typedoc "A map of states to non-empty lists of {predicate, target_state} tuples."
  @type transitions :: %{valid_state() => [{predicate(), valid_state()}, ...]}

  @enforce_keys [:initial, :finals, :transitions]
  defstruct [:initial, :finals, :transitions]

  @typedoc "Create a new automaton with initial, final state(s), and transitions."
  @type t :: %Automaton{
    initial: valid_state(),
    finals: [valid_state(), ...],
    transitions: transitions()
  }

  @doc """
  Constructor to create a new 'Automaton'. Entails an initial state, final state(s) and transitions.
  """
  @spec new(initial :: valid_state(), finals :: [valid_state(), ...], transitions :: transitions()) :: t()
  def new(initial, finals, transitions) do
  %Automaton{
      initial: initial,
      finals: finals,
      transitions: transitions
    }
  end
end

tokens = [
      # Defines the list of tokens recognized by the lexer.
      # - Brackets / delimiters
      Token.new(:begin_block, Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]})),
      Token.new(:end_block,   Automaton.new(0, [1], %{0 => [{&(&1 == ?}), 1}]})),
      Token.new(:begin_par,   Automaton.new(0, [1], %{0 => [{&(&1 == ?(), 1}]})),
      Token.new(:end_par,     Automaton.new(0, [1], %{0 => [{&(&1 == ?)), 1}]})),
      Token.new(:semicolon,   Automaton.new(0, [1], %{0 => [{&(&1 == ?;), 1}]})),

      # - Operators
      Token.new(:op_eg,       Automaton.new(0, [2], %{0 => [{&(&1 == ?=), 1}], 
                                                      1 => [{&(&1 == ?=), 2}]})),
      Token.new(:op_affect,   Automaton.new(0, [1], %{0 => [{&(&1 == ?=), 1}]})),
      Token.new(:op_add,      Automaton.new(0, [1], %{0 => [{&(&1 == ?+), 1}]})),
      Token.new(:op_minus,    Automaton.new(0, [1], %{0 => [{&(&1 == ?-), 1}]})),
      Token.new(:op_mult,     Automaton.new(0, [1], %{0 => [{&(&1 == ?*), 1}]})),
      Token.new(:op_div,      Automaton.new(0, [1], %{0 => [{&(&1 == ?/), 1}]})),

      # - Keywords
      Token.new(:type_int,    Automaton.new(0, [3], %{0 => [{&(&1 == ?i), 1}], 
                                                      1 => [{&(&1 == ?n), 2}], 
                                                      2 => [{&(&1 == ?t), 3}]})),
      Token.new(:cond,        Automaton.new(0, [2], %{0 => [{&(&1 == ?i), 1}], 
                                                      1 => [{&(&1 == ?f), 2}]})),
      Token.new(:loop,        Automaton.new(0, [5], %{0 => [{&(&1 == ?w), 1}], 
                                                      1 => [{&(&1 == ?h), 2}], 
                                                      2 => [{&(&1 == ?i), 3}], 
                                                      3 => [{&(&1 == ?l), 4}], 
                                                      4 => [{&(&1 == ?e), 5}]})),
      # - Literals / identifiers
      Token.new(:value_int,   Automaton.new(0, [1], %{0 => [{&(&1 in ?0..?9), 1}], 
                                                      1 => [{&(&1 in ?0..?9), 1}]})),
      Token.new(:ident,       Automaton.new(0, [1], %{0 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 == ?_), 1}], 
                                                      1 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 in ?0..?9 or &1 == ?_), 1}]}))
]DeepL was used in places to translate the personal reflection.

defmodule Lexer do
  @moduledoc """
  A lexical analyzer (lexer) for tokenizing text based on a set of defined tokens.

  This module provides functions to recognize tokens in a text using 
  finite automata associated with each token. It supports:

    * 'next_state/3'
    * 'recognized_from_state/4'
    * 'fetch_recognized_token/2'
    * 'lex_analysis!/2'
  """

  @doc """
  Computes the next states of an automaton given a current 'state' and input character 'c'.
  
  Returns a list of states reachable from 'state' via transitions whose predicate
  matches 'c'. Returns an empty list if no transitions exist for the given character.
  """
  @spec next_state(state :: Automaton.valid_state(), c :: char(), automaton :: Automaton.t()) :: [Automaton.valid_state()]
  def next_state(state, c, automaton) do
    transitions = Map.get(automaton.transitions , state, []) # get transitions of automaton

    # list comprehension, filtered: only keep tuples if predicate.(c) returns true
    for {predicate, next_state} <- transitions, predicate.(c) do
      next_state
    end
  end

  @doc """
  Attempts to recognize a token from the given 'state' and 'text' using the token's automaton.
  
  Returns a quadruple '{:ok, {token_code, recognized_chars}, length, rest}' where:
  
    * 'token_code' — the code of the recognized token
    * 'recognized_chars' — the list of characters matched
    * 'length' — the number of characters consumed
    * 'rest' — the remaining unparsed text
  
  Otherwise it returns ':error' if no characters are matched.
  """
  @spec recognized_from_state(state :: Automaton.valid_state(),text :: [char()],acc :: {list(char()), non_neg_integer()},token :: Token.t()) ::
        {:ok, {Token.code(), [char()]}, non_neg_integer(), [char()]} | :error
  def recognized_from_state(state, text, {rec, len}, token) do
    {_, rec, length} =
      Enum.reduce_while(text, {state, rec, len}, fn char, {state, rec, len} ->
        next = next_state(state, char, token.automaton) # get next state 
          
        if next == [] do
          {:halt, {state, rec, len}}                    # stop immediately when next is empty...
        else
          #IO.inspect("Process char #{<<char>>} -> next state: #{hd(next)}")
          {:cont, {hd(next), rec ++ [char], len + 1}}   # ... otherwise continue accumulating
        end
      end)
    
    rest = Enum.drop(text, length)                      # return the rest of the unparsed text
    
    if length == 0 do                                   # return :error if nothing is parsed
      :error
    else
      {:ok, { token.code, List.to_string(rec) }, length, rest}          # otherwise return resulting quadruple
    end
  end

  
  @doc """
  Finds the token from the given list of 'tokens' that recognizes the longest
  prefix of 'text'.  
  
  Returns '{:ok, {token_code, recognized_chars}, length, rest}' if a token is recognized,
  or ':error' if no token matches.
  """
  @spec fetch_recognized_token(text :: [char()],tokens :: [Token.t()]) ::
        {:ok, {Token.code(), [char()]}, non_neg_integer(), [char()]} | :error
  def fetch_recognized_token(text, tokens) do
      Enum.map(tokens, fn token ->        # invoke recognized_from_state on every token 
        recognized_from_state(0, text, {~c"", 0}, token)
        end)
      |> Enum.filter(fn
        {:ok, _, _, _} -> true            # filter out only successful tokens
        _ -> false
      end)
      |> Enum.sort_by(fn {:ok, _, length, _rest} -> length end, &>=/2) # sort tokens by length (desc)
      |> List.first()                     # take the first and thus longest token
      |> (fn                             # when tuple is not nil: remove length & shorten tuple
        nil -> nil
        {a, {b1, b2}, _, d} -> {a, {b1, b2}, d}
      end).()
      |> case do
       nil -> :error                      # return :error when result is empty ...
       matches -> matches                 # ... otherwise found results (=matches)     
     end
  end

  @doc """
  Recursively tokenizes the input 'text' using the list of 'tokens'.
  
  Converts the input string to a charlist, repeatedly finds the longest
  matching token at the beginning of the text, and returns a flat list
  of token codes and their recognized string values.
  """
  @spec lex_analysis!(text :: String.t(), tokens :: [Token.t()]) :: [Token.code() | String.t()]
  def lex_analysis!("", _), do: []
  def lex_analysis!(text, tokens) do
    text = String.to_charlist(String.trim(text))
    {_,{ident, rec}, _, rest} = fetch_recognized_token(text, tokens) # fetch
    [ident, List.to_string(rec) | lex_analysis!(List.to_string(rest), tokens)] # and recurse rest
  end
end
```

```elixir
defmodule LexerDoctest do

  # It's necessary to expose tokens via function as doctest has special needs.
  def tokens do
    [
    # Brackets / delimiters
    Token.new(:begin_block, Automaton.new(0, [1], %{0 => [{&(&1 == ?{), 1}]})),
    Token.new(:end_block,   Automaton.new(0, [1], %{0 => [{&(&1 == ?}), 1}]})),
    Token.new(:begin_par,   Automaton.new(0, [1], %{0 => [{&(&1 == ?(), 1}]})),
    Token.new(:end_par,     Automaton.new(0, [1], %{0 => [{&(&1 == ?)), 1}]})),
    Token.new(:semicolon,   Automaton.new(0, [1], %{0 => [{&(&1 == ?;), 1}]})),

    # Operators
    Token.new(:op_eg,       Automaton.new(0, [2], %{0 => [{&(&1 == ?=), 1}], 1 => [{&(&1 == ?=), 2}]})),
    Token.new(:op_affect,   Automaton.new(0, [1], %{0 => [{&(&1 == ?=), 1}]})),
    Token.new(:op_add,      Automaton.new(0, [1], %{0 => [{&(&1 == ?+), 1}]})),
    Token.new(:op_minus,    Automaton.new(0, [1], %{0 => [{&(&1 == ?-), 1}]})),
    Token.new(:op_mult,     Automaton.new(0, [1], %{0 => [{&(&1 == ?*), 1}]})),
    Token.new(:op_div,      Automaton.new(0, [1], %{0 => [{&(&1 == ?/), 1}]})),

    # Keywords
    Token.new(:type_int,    Automaton.new(0, [3], %{0 => [{&(&1 == ?i), 1}], 1 => [{&(&1 == ?n), 2}], 2 => [{&(&1 == ?t), 3}]})),
    Token.new(:cond,        Automaton.new(0, [2], %{0 => [{&(&1 == ?i), 1}], 1 => [{&(&1 == ?f), 2}]})),
    Token.new(:loop,        Automaton.new(0, [5], %{0 => [{&(&1 == ?w), 1}], 1 => [{&(&1 == ?h), 2}], 2 => [{&(&1 == ?i), 3}], 3 => [{&(&1 == ?l), 4}], 4 => [{&(&1 == ?e), 5}]})),

    # Literals / identifiers
    Token.new(:value_int,   Automaton.new(0, [1], %{0 => [{&(&1 in ?0..?9), 1}], 1 => [{&(&1 in ?0..?9), 1}]})),
    Token.new(:ident,       Automaton.new(0, [1], %{0 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 == ?_), 1}], 1 => [{&(&1 in ?A..?Z or &1 in ?a..?z or &1 in ?0..?9 or &1 == ?_), 1}]}))
    ] end

  @moduledoc ~S"""
  Module 'LexerDoctest' tests Lexer functions thoroughly.
  Doctests for Lexer module - taken from the specifications and 
  then added additional border line cases. Short explanation to be found at every test.
    
    # Recognise simple ident
      iex> Lexer.recognized_from_state(0, ~c"toto=3;", {~c"", 0}, Enum.at(LexerDoctest.tokens(),15))
      {:ok, {:ident, "toto"}, 4, ~c"=3;"}

    # Recognise an error if ident block contains unallowed chars
      iex> Lexer.recognized_from_state(0, ~c"{x=3;}", {~c"", 0}, Enum.at(LexerDoctest.tokens(),15))
      :error

    # Recognise proper begin block
      iex> Lexer.recognized_from_state(0, ~c"{", {~c"", 0}, Enum.at(LexerDoctest.tokens(),0))
      {:ok, {:begin_block, "{"}, 1, []}

    # Has to recognise {:ok, {:loop, "while"}, ~c" (x==3)"} and not {:ok, {:ident, "while"}, ~c" (x==3)"}.
      iex> Lexer.fetch_recognized_token(~c"while (x==3)", LexerDoctest.tokens())
      {:ok, {:loop, "while"}, ~c" (x==3)"}
    
    # Recognition of unproper begin block
      iex> Lexer.recognized_from_state(0, ~c"{abc", {~c"", 0}, Enum.at(LexerDoctest.tokens(),0))
      {:ok, {:begin_block, "{"}, 1, ~c"abc"}

    # Recognition of unproper end block
      iex> Lexer.recognized_from_state(0, ~c"}toto=3;", {~c"", 0}, Enum.at(LexerDoctest.tokens(),1))
      {:ok, {:end_block, "}"}, 1, ~c"toto=3;"}

    #DeepL was used in places to translate the personal reflection. Recognise unallowed chars
      iex> Lexer.recognized_from_state(0, ~c"#abc", {~c"", 0}, Enum.at(LexerDoctest.tokens(),0))
      :error

    # Fetch proper ident token
      iex> Lexer.fetch_recognized_token(~c"toto", LexerDoctest.tokens())
      {:ok, {:ident, "toto"}, []}

    # Fetch begin block
      iex> Lexer.fetch_recognized_token(~c"{x=3;}", LexerDoctest.tokens())
      {:ok, {:begin_block, "{"}, ~c"x=3;}"}

    # Throw an error when fetching begins with ":"
      iex> Lexer.fetch_recognized_token(~c": x=3;", LexerDoctest.tokens())
      :error

    # Test order of processing the tokens by fetching triplet {:ok, {:loop, "while"}, ~c" (x==3)"} and
    # not triplet {:ok, {:ident, "while"}, ~c" (x==3)"}
      iex> Lexer.fetch_recognized_token(~c"while (x==3)", LexerDoctest.tokens())
      {:ok, {:loop, "while"}, ~c" (x==3)"}

    # Whereas a longer word is treated first
      iex> Lexer.fetch_recognized_token(~c"whileX (x==3)", LexerDoctest.tokens())
      {:ok, {:ident, "whileX"}, ~c" (x==3)"}
    
  """
end
```

### Retour théorique personnel

* Expliquez et justifiez les choix d’implémentation concernant vos fonctions
  pour cette dernière version du projet. Proposer des possibles ouvertures
  afin de continuer ce projet, ou pour l’adapter à l’analyse d’un autre
  langage. (~250 mots)
* Faites un retour réflexif sur le langage Elixir (ses points forts / faibles, les
  techniques propre à la programmation fonctionnelle, …). Comparez avec
  des langages et paradigmes que vous connaissez déjà. (~250 mots)

<!-- livebook:{"break_markdown":true} -->

**Implementation decisions & lookout**

My implementation decisions followed the specifications. Basically, I used a basic test-driven development technique, with which I attempted to fulfill the corresponding parsing tasks of the specification. The specifications already included the most difficult edge cases and were therefore ideal. I tried to write the methods as self-explanatory and simple as possible, which was not always successful due to the complexity. Enumerations and maps were used often and the language's inherent ability for direct pattern matching was also very practical. The use of pipes was particularly beneficial for readability.

The lexer can now recognise and analyse the structure of code to a certain extent. Now, of course, it would be interesting to expand this syntactic checking further, such as the following features:
Every bracket that is opened must be closed again by its counterpart. Commands must always be terminated by a semicolon. Certain blocks could be indented automatically. Character strings must be correctly enclosed in quotation marks. Comment characters suppress the interpretation of certain areas. In addition to this syntactic structure, a semantic one could be implemented. For example, data types could be checked: Does an integer contain only numbers?

I have not reached a conclusive decision regarding the porting of this project and/or its continuation in another language. If we consider a lexer that performs syntactic-semantic checking we must assume a large number of characters to be parsed. Due to constant user input parsing must be done continuously. For these reasons - even if the processing still needs to be greatly optimised - Elixir would be a good choice due to its speed and low memory usage. Languages closer to the system, like C, would likely offer better performance but would demand more effort to implement the lexer functionality.

**Language reflections**

Elixir offers powerful tools for iteratively processing lists (continues passing, accumulators). This is ideal for processing data streams as a lexer must be able to do.

Recursive solutions have both advantages and disadvantages: on the one hand, a recursive function can be very elegant and contain little redundant code, with an accumulator gathering all the necessary information along the way. However, as soon as errors occur during processing, it becomes somewhat difficult in this functional context to identify exactly when the error occurred, since we are not actually working with defined states. The lack of simple debugging tools exacerbates this difficulty:
We can surely use IO.inspect to examine objects at runtime with a reasonable degree of flexibility. But the included debugger 'pry' can only be used on the command line and, in my opinion, is not very convenient (unless you know exactly where the error is likely to be). Even the most frequently used plugin, 'ElixirLS' in vscode, does not support sophisticated debugging. These circumstances make debugging unnecessarily time-consuming. Every other high-level programming language I know (C, C#, Python, Ruby, Java) comes with a graphical debugger in IDEs such as vscode or IntelliJ.
I also noticed the dynamic typing of the language when creating this lexer: while prototyping can be written fairly quickly, runtime errors can occur that are difficult to track down (see debugging options).

Despite these weaknesses, I do of course also see the strengths of functional programming. Modules that are kept as small as possible, ideally delivering ‘pure’, idempotent results that can ultimately be strung together: these are architectural characteristics whose manifestations we recognise in microservices and Rest APIs, in Unix-like kernels or pipe systems. And anonymous functions are used extremely frequently in other high-level languages such as Python or C#. And, of course, it makes sense in terms of resources and security to use as little memory as possible. Furthermore, since Elixir runs on the Erlang Virtual Machine BEAM, we would not have to compile our lexer separately for each target system. 

Therefore, I believe that the decision for a specific paradigm must be adapted to the situation and environment and cannot be a black-and-white question.
